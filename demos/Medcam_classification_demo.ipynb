{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Gcam classification demo.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC1x79crLbWG",
    "colab_type": "text"
   },
   "source": [
    "# **Using M3d-CAM for classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "412fWaVFLi5n",
    "colab_type": "text"
   },
   "source": [
    "In this demo you will learn how to use M3d-CAM for classification using a resnet152. We will use a subset of the famous [Cats vs Dogs Dataset](https://github.com/Karol-G/gcam_cat_dog_examples) for this demo. \\\\\n",
    "\n",
    "This demonstration was made using Google Colab and probably won't work if you are not using Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULVBcN6rNYKj",
    "colab_type": "text"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AjINkJ-Na4T",
    "colab_type": "text"
   },
   "source": [
    "Clone the Cats vs Dogs repository and set up the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6WgKnOr4vgJ8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!git clone https://github.com/Karol-G/Gcam_cat_dog_examples.git"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXW6az_9Otqd",
    "colab_type": "text"
   },
   "source": [
    "Install M3d-CAM:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z3QabwbOOuP9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "pip install medcam"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRnRCxG6O8Ku",
    "colab_type": "text"
   },
   "source": [
    "# Model & dataloader setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51ExmwZVQky5",
    "colab_type": "text"
   },
   "source": [
    "Next we set up our resnet152 model and the dataloader for loading the cat and dog images:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B6ATUSqGPPY8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "# Setup the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.resnet152(pretrained=True)\n",
    "model.to(device=device)\n",
    "model.eval()\n",
    "\n",
    "def load_image(image_path):\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    raw_image = cv2.resize(raw_image, (224,) * 2)\n",
    "    image = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )(raw_image[..., ::-1].copy())\n",
    "    image = image.to(device)\n",
    "    return image\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ImageFolder('/content/Gcam_cat_dog_examples/dataset', loader=load_image)\n",
    "# Set up the dataloader\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_t16-CHQpOw",
    "colab_type": "text"
   },
   "source": [
    "# Injecting M3d-CAM into resnet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYHTpfmjQt_L",
    "colab_type": "text"
   },
   "source": [
    "The beauty of M3d-CAM is that you only need to insert a single line of code (or two if you count the import) for everything to work:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sokv7ECZPcB3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from medcam import medcam\n",
    "\n",
    "model = medcam.inject(model, output_dir='attention_maps', backend='gcam', layer='layer4', label='best', save_maps=True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay2eZJuSXsje",
    "colab_type": "text"
   },
   "source": [
    "After your model is injected with M3d-CAM it will still behave as it would normally do. So even if you have a big and complex project nothing will break and it will run as it always did. \\\\\n",
    "The only difference is that every time the `model.forward()` of your model is called attention maps will be generated for your current input and automatically saved to `output_dir`. \\\\\n",
    "The output of your model stays the same as before the injection. (Of course you can change this behavior and return the attention maps instead by setting `replace=True` during the injection).\n",
    "\n",
    "Now to generate some attention maps we will call the `model.forward()` with the cat and dog images a couple times:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Obfr3Xk8aAAQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "for batch in data_loader:\n",
    "    _ = model(batch[0])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nst34bUteTYk",
    "colab_type": "text"
   },
   "source": [
    "Now you can display the generated attention maps in colab with:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BEsgyHFKeaO2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/content/attention_maps/layer4/attention_map_0_0_0.png')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0yxK-7Tep0R",
    "colab_type": "text"
   },
   "source": [
    "# Some further notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTJB4FCbRuDD",
    "colab_type": "text"
   },
   "source": [
    "As M3d-CAM offers multiple methods of visualization (*backends*) you can simply change the backend keyword to one of the following: \\\\\n",
    "- *gbp* (Guided Backpropagation)\n",
    "- *gcam* (Grad-Cam, default)\n",
    "- *ggcam* (Guided Grad-Cam)\n",
    "- *gcampp* (Grad-Cam++)\n",
    "\n",
    "The layer keyword tells M3d-CAM for which layer the attention maps should be generated. You can also set the layer to 'auto' (the default setting) and M3d-CAM will choose the last convolutional layer which is what you want in most cases. However this is still experimental and won't always choose the correct layer. But it works in most cases. \\\\\n",
    "You can print all layers of a model with `medcam.get_layers(model)` if you don't know the layer names. However you cannot generate attention maps from every layer. \\\\\n",
    "\n",
    "Furthermore the label keyword tells M3d-CAM on which class label it should focus. The default setting is 'best' which always selects the class with the highest probability. Alternativly you can set the label manually as a number."
   ]
  }
 ]
}